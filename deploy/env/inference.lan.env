# Inference node LAN profile
# Host target: Ryzen 7 2700X / 62GB / RTX 3050 8GB
# Place as: /home/user/leapFrog/inference-node/.env

INF_HOST=0.0.0.0
INF_PORT=9000

# Local llama.cpp server endpoint on inference node
LLAMA_SERVER_URL=http://127.0.0.1:8080

# Update these paths to match your inference host
LLAMA_CPP_SERVER_BIN=/opt/llama.cpp/build/bin/llama-server
LLAMA_MODEL_PATH=/models/qwen2.5-14b-instruct-q4_k_m.gguf

# Initial tuning for 8GB VRAM
LLAMA_CTX_SIZE=12288
LLAMA_N_GPU_LAYERS=33
LLAMA_THREADS=12
LLAMA_BATCH=512

# Usually true under systemd so llama-server starts with inference API
AUTO_LAUNCH_LLAMA=true
